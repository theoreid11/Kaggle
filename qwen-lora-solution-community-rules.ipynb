{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":426330,"sourceType":"modelInstanceVersion","modelInstanceId":347541,"modelId":368803}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile constants.py\nBASE_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\nLORA_PATH = \"output/\"\nCOMPLETE = \"Answer:\"\nprompt = f\"You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Answer 'yes' or 'no' only.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:27:07.840709Z","iopub.execute_input":"2025-09-25T23:27:07.840924Z","iopub.status.idle":"2025-09-25T23:27:07.850029Z","shell.execute_reply.started":"2025-09-25T23:27:07.840899Z","shell.execute_reply":"2025-09-25T23:27:07.849281Z"}},"outputs":[{"name":"stdout","text":"Writing constants.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\ntrain_data = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\nprint(len(train_data))\ntest_sample=    pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\").sample(frac=0.5,random_state = 42 )\n\nprint(len(test_sample))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch import Tensor\nimport kagglehub\n\n\n# Pooling function (same as before)\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n    # Check whether padding is on the left\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\n\n# OOP wrapper around tokenizer + model + pooling\nclass QwenEmbedder(nn.Module):\n    def __init__(self, model_dir: str, max_length: int = 8192, device: str = None):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, padding_side=\"left\")\n        self.model = AutoModel.from_pretrained(model_dir)\n        self.max_length = max_length\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.to(self.device)  # move model to device\n\n    def forward(self, texts: list[str]) -> Tensor:\n        # Tokenize batch\n        batch_dict = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        ).to(self.device)\n\n        # Forward pass\n        outputs = self.model(**batch_dict)\n\n        # Pool to sequence embedding\n        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n\n        # Normalize embeddings\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n        return embeddings\n\nif __name__ == \"__main__\":\n    model_dir = kagglehub.model_download(\"qwen-lm/qwen-3-embedding/transformers/0.6b\")\n    embedder = QwenEmbedder(model_dir)\n\n    # Queries and docs\n    queries = [\n        \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: What is the capital of China?\",\n        \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: Explain gravity\"\n    ]\n    documents = [\n        \"The capital of China is Beijing.\",\n        \"Gravity is a force that attracts two bodies towards each other...\"\n    ]\n\n    # Get embeddings\n    query_emb = embedder(queries)\n    doc_emb = embedder(documents)\n\n    # Compute similarity\n    scores = query_emb @ doc_emb.T\n    print(scores.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\n\nimport pandas as pd \nfrom constants import prompt, COMPLETE\nimport numpy as np \nimport random\nimport re\nfrom datasets import Dataset\n\nrandom.seed(42)\nnp.random.seed(42)\n\ndef url_to_semantics(txt : str) -> str:\n    if not isinstance(txt,str):\n        return \"\"\n    url_pattern = r'https?://[^\\s/$.?#].[^\\s]*'\n    urls = re.findall(url_pattern, txt)\n    \n    if not urls:\n        return \"\" \n\n    all_semantics = []\n    seen_semantics = set()\n\n    for url in urls:\n        url_lower = url.lower()\n        \n        domain_match = re.search(r\"(?:https?://)?([a-z0-9\\-\\.]+)\\.[a-z]{2,}\", url_lower)\n        if domain_match:\n            full_domain = domain_match.group(1)\n            parts = full_domain.split('.')\n            for part in parts:\n                if part and part not in seen_semantics and len(part) > 3: # Avoid short parts like 'www'\n                    all_semantics.append(f\"domain:{part}\")\n                    seen_semantics.add(part)\n\n        # Extract path parts\n        path = re.sub(r\"^(?:https?://)?[a-z0-9\\.-]+\\.[a-z]{2,}/?\", \"\", url_lower)\n        path_parts = [p for p in re.split(r'[/_.-]+', path) if p and p.isalnum()] # Split by common delimiters\n\n        for part in path_parts:\n            # Clean up potential file extensions or query params\n            part_clean = re.sub(r\"\\.(html?|php|asp|jsp)$|#.*|\\?.*\", \"\", part)\n            if part_clean and part_clean not in seen_semantics and len(part_clean) > 3:\n                all_semantics.append(f\"path:{part_clean}\")\n                seen_semantics.add(part_clean)\n\n    if not all_semantics:\n        return \"\"\n\n    return f\"\\nURL Keywords: {' '.join(all_semantics)}\"\n\ndef build_prompt(row):\n    subreddit = row.get(\"subreddit\", \"unknown\")\n    rule = row.get(\"rule\", \"\")\n    pos_example = row.get(\"positive_example\", \"\")\n    neg_example = row.get(\"negative_example\", \"\")\n    body = row.get(\"body\", \"\")\n    url_features_body = url_to_semantics(body)\n    url_features_pos = url_to_semantics(pos_example)\n    url_features_neg = url_to_semantics(neg_example)\n    return f\"\"\"\n{prompt}\n\nr/{subreddit} \nrule: {rule}\nExamples : \n1) {pos_example}{url_features_pos}\n{COMPLETE} yes\n2) {neg_example}{url_features_neg}\n{COMPLETE} no\n\n------\nComment: {body}{url_features_body}\n{COMPLETE} \"\"\"\n\n\ndef get_data_for_training(fpath,sample_frac = 0.5):\n    train_data = pd.read_csv(f\"{fpath}/train.csv\")\n    \n    test_df= pd.read_csv(f\"{fpath}/test.csv\").sample(frac=sample_frac,random_state = 42 )\n\n    \n\n\n    train_df = train_data[['body','rule','subreddit','positive_example_1','positive_example_2', 'negative_example_1','negative_example_2','rule_violation']]\n    \n    #randomly assign examples\n    train_df['positive_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['positive_example_1'],train_df['positive_example_2'])\n    train_df['negative_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['negative_example_1'], train_df['negative_example_2'])\n    train_df.drop(columns = ['positive_example_1','positive_example_2', 'negative_example_1','negative_example_2'], inplace = True)\n\n    dfs = [train_df]\n    \n    # build test df \n    \n    for rule_violation in ['yes', 'no']:\n        for i in range(1,3): #loop through both examples\n            subdf =  test_df.copy().drop(columns=['body','positive_example_1','positive_example_2', 'negative_example_1','negative_example_2'])\n\n            if rule_violation == 'yes':   # case when rule is violated \n                subdf['body'] = test_df[f'positive_example_{i}']\n                subdf['positive_example'] = test_df[f'positive_example_{3-i}']\n                subdf['negative_example'] = np.where(np.random.rand(len(test_df))<0.5, test_df[f'negative_example_{i}'],test_df[f'negative_example_{3-i}'])\n                subdf['rule_violation'] = 1\n            else:  # case when rule is not violated \n                subdf['body'] = test_df[f'negative_example_{i}']\n                subdf['positive_example'] = np.where(np.random.rand(len(test_df))<0.5, test_df[f'positive_example_{i}'],test_df[f'positive_example_{3-i}'])\n\n                subdf['neagtive_example'] = test_df[f'negative_example_{3-i}']\n                subdf['rule_violation'] = 0\n            dfs.append(subdf)\n\n    df =  pd.concat(dfs, axis = 0).drop_duplicates(ignore_index = True)\n    \n    return df\n\ndef build_dataset(df):\n    df['prompt'] = df.apply(build_prompt, axis = 1)\n\n    df['completion'] = df['rule_violation'].map(\n        {\n            1 : 'yes',\n            0 : 'no'\n        }\n    )\n    df = df[['prompt','completion']]\n\n    print(df)\n\n    dataset = Dataset.from_pandas(df)\n    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:27:19.979333Z","iopub.execute_input":"2025-09-25T23:27:19.979647Z","iopub.status.idle":"2025-09-25T23:27:19.987182Z","shell.execute_reply.started":"2025-09-25T23:27:19.979623Z","shell.execute_reply":"2025-09-25T23:27:19.986091Z"}},"outputs":[{"name":"stdout","text":"Writing utils.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:27:36.540691Z","iopub.execute_input":"2025-09-25T23:27:36.541443Z","iopub.status.idle":"2025-09-25T23:29:13.833248Z","shell.execute_reply.started":"2025-09-25T23:27:36.541418Z","shell.execute_reply":"2025-09-25T23:29:13.832425Z"}},"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.8.1)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.6.0)\nCollecting transformers>=4.56.1 (from trl)\n  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.6.0+cu124)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.33.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nCollecting huggingface_hub>=0.21.0 (from accelerate>=1.4.0->trl)\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.56.1->trl) (2024.11.6)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.6.15)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=1.4.0->trl)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=1.4.0->trl) (2024.2.0)\nDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, tokenizers, nvidia-cusolver-cu12, transformers, trl\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.4\n    Uninstalling transformers-4.52.4:\n      Successfully uninstalled transformers-4.52.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 huggingface_hub-0.35.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.22.1 transformers-4.56.2 trl-0.23.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:29:15.377510Z","iopub.execute_input":"2025-09-25T23:29:15.377811Z","iopub.status.idle":"2025-09-25T23:29:46.889109Z","shell.execute_reply.started":"2025-09-25T23:29:15.377783Z","shell.execute_reply":"2025-09-25T23:29:46.888525Z"}},"outputs":[{"name":"stderr","text":"2025-09-25 23:29:27.589421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758842967.797880      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758842967.857387      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:29:54.669249Z","iopub.execute_input":"2025-09-25T23:29:54.669600Z","iopub.status.idle":"2025-09-25T23:30:00.964417Z","shell.execute_reply.started":"2025-09-25T23:29:54.669577Z","shell.execute_reply":"2025-09-25T23:30:00.963575Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.47.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile train.py\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display, HTML\nfrom utils import get_data_for_training, build_dataset, build_prompt, url_to_semantics\n\n# Lora imports\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom tqdm.auto import tqdm\nfrom transformers.utils import is_torch_bf16_gpu_available\nfrom constants import LORA_PATH, BASE_MODEL_PATH\n\n\ndef main():\n    data_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n    df = get_data_for_training(data_path)\n    train_dataset = build_dataset(df)\n    df_train = pd.DataFrame(train_dataset)\n\n    df_train = pd.DataFrame(train_dataset)\n\n    lora_config = LoraConfig(\n        r = 16,\n        lora_alpha= 32,\n        lora_dropout = 0.1,\n        bias = \"none\",\n        target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type = \"CAUSAL_LM\")\n    \n    training_args = SFTConfig(\n        num_train_epochs = 1,\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 4,\n        optim = \"paged_adamw_8bit\",\n        learning_rate = 5e-5,\n        weight_decay = 0.01,\n        max_grad_norm = 1.0,\n\n        lr_scheduler_type = \"cosine\",\n        warmup_ratio=0.05,\n\n        bf16= is_torch_bf16_gpu_available(),\n        fp16=not is_torch_bf16_gpu_available(),\n        dataloader_pin_memory=True,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n\n        save_strategy= \"no\",\n        report_to = \"none\",\n\n        completion_only_loss = True,\n        packing = True,\n        remove_unused_columns = False\n    )\n    \n    trainer = SFTTrainer(\n        BASE_MODEL_PATH,\n        args = training_args,\n        train_dataset = train_dataset,\n        peft_config = lora_config\n    )\n\n    trainer.train()\n    trainer.save_model(LORA_PATH)\n    #print(df_train.head(10))\n    \n\nif __name__ == \"__main__\":\n    main()\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:30:59.221975Z","iopub.execute_input":"2025-09-25T23:30:59.222827Z","iopub.status.idle":"2025-09-25T23:30:59.230037Z","shell.execute_reply.started":"2025-09-25T23:30:59.222791Z","shell.execute_reply":"2025-09-25T23:30:59.229067Z"}},"outputs":[{"name":"stdout","text":"Writing train.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install vllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T00:16:14.651800Z","iopub.execute_input":"2025-09-26T00:16:14.652100Z","iopub.status.idle":"2025-09-26T00:20:59.055396Z","shell.execute_reply.started":"2025-09-26T00:16:14.652074Z","shell.execute_reply":"2025-09-26T00:20:59.054535Z"}},"outputs":[{"name":"stdout","text":"Collecting vllm\n  Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from vllm) (2024.11.6)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (7.0.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.4)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\nCollecting blake3 (from vllm)\n  Downloading blake3-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (217 bytes)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: transformers>=4.55.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.56.2)\nRequirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.22.1)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (3.20.3)\nRequirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.13)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.12.13)\nCollecting openai>=1.99.1 (from vllm)\n  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.7)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.22.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\nCollecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\nCollecting lm-format-enforcer==0.11.3 (from vllm)\n  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\nCollecting llguidance<0.8.0,>=0.7.11 (from vllm)\n  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting outlines_core==0.2.11 (from vllm)\n  Downloading outlines_core-0.2.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\nCollecting diskcache==5.6.3 (from vllm)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nCollecting lark==1.2.2 (from vllm)\n  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting xgrammar==0.1.23 (from vllm)\n  Downloading xgrammar-0.1.23-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.14.0)\nRequirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\nCollecting partial-json-parser (from vllm)\n  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\nCollecting pyzmq>=25.0.0 (from vllm)\n  Downloading pyzmq-27.1.0-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)\nCollecting msgspec (from vllm)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting gguf>=0.13.0 (from vllm)\n  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm)\n  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\nCollecting compressed-tensors==0.11.0 (from vllm)\n  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\nCollecting depyf==0.19.0 (from vllm)\n  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\nCollecting watchfiles (from vllm)\n  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.3)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\nCollecting pybase64 (from vllm)\n  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\nCollecting cbor2 (from vllm)\n  Downloading cbor2-5.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from vllm) (1.3.6)\nCollecting openai-harmony>=0.0.3 (from vllm)\n  Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\nCollecting numba==0.61.2 (from vllm)\n  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nCollecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm)\n  Downloading ray-2.49.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (21 kB)\nCollecting torch==2.8.0 (from vllm)\n  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\nCollecting torchaudio==2.8.0 (from vllm)\n  Downloading torchaudio-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\nCollecting torchvision==0.23.0 (from vllm)\n  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nCollecting xformers==0.0.32.post1 (from vllm)\n  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from compressed-tensors==0.11.0->vllm) (2.4.6)\nCollecting astor (from depyf==0.19.0->vllm)\n  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.19.0->vllm) (0.3.8)\nCollecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm)\n  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer==0.11.3->vllm) (25.0)\nCollecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nCollecting sympy>=1.13.3 (from torch==2.8.0->vllm)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->vllm) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->vllm) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->vllm) (2025.3.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0->vllm)\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0->vllm)\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0->vllm)\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0->vllm)\n  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0->vllm)\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0->vllm)\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0->vllm)\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0->vllm)\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0->vllm)\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.8.0->vllm)\n  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\nCollecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0->vllm)\n  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0->vllm)\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0->vllm)\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0->vllm)\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.4.0 (from torch==2.8.0->vllm)\n  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch==2.8.0->vllm) (75.2.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.2)\nCollecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi_cli-0.0.13-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\nRequirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.3)\nRequirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.24.0)\nCollecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2.4.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.99.1->vllm) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.99.1->vllm) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.99.1->vllm) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.99.1->vllm) (1.3.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->vllm) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->vllm) (0.4.1)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (8.2.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.1)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.6.15)\nRequirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.21.1->vllm) (0.35.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.55.2->vllm) (0.5.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.1)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\nRequirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.16.0)\nCollecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading rich_toolkit-0.15.1-py3-none-any.whl.metadata (1.0 kB)\nCollecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi_cloud_cli-0.2.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm) (1.1.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.8.0->vllm) (3.0.2)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.25.1)\nCollecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.8.0->vllm) (1.3.0)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\nRequirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.5.0.post1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vllm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vllm) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vllm) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vllm) (2024.2.0)\nCollecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading rignore-0.6.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.31.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vllm) (2024.2.0)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.0.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.17.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.22)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\nDownloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl (436.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading compressed_tensors-0.11.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading depyf-0.19.0-py3-none-any.whl (39 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading outlines_core-0.2.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchaudio-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xgrammar-0.1.23-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m105.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading openai-1.109.1-py3-none-any.whl (948 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\nDownloading pyzmq-27.1.0-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (857 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.0/857.0 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ray-2.49.2-cp311-cp311-manylinux2014_x86_64.whl (70.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading blake3-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cbor2-5.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (254 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\nDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi_cli-0.0.13-py3-none-any.whl (11 kB)\nDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\nDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nDownloading fastapi_cloud_cli-0.2.1-py3-none-any.whl (19 kB)\nDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading rich_toolkit-0.15.1-py3-none-any.whl (29 kB)\nDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading rignore-0.6.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (950 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.6/950.6 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, uvloop, triton, sympy, rignore, pyzmq, python-dotenv, pycountry, pybase64, partial-json-parser, outlines_core, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, llvmlite, llguidance, lark, interegular, httptools, diskcache, cbor2, blake3, astor, watchfiles, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, depyf, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, openai, nvidia-cusolver-cu12, lm-format-enforcer, torch, ray, fastapi-cloud-cli, fastapi-cli, torchaudio, mistral_common, xgrammar, xformers, torchvision, numba, gguf, compressed-tensors, vllm\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: pyzmq\n    Found existing installation: pyzmq 24.0.1\n    Uninstalling pyzmq-24.0.1:\n      Successfully uninstalled pyzmq-24.0.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.5.147\n    Uninstalling nvidia-curand-cu12-10.3.5.147:\n      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n  Attempting uninstall: openai\n    Found existing installation: openai 1.91.0\n    Uninstalling openai-1.91.0:\n      Successfully uninstalled openai-1.91.0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: ray\n    Found existing installation: ray 2.47.1\n    Uninstalling ray-2.47.1:\n      Successfully uninstalled ray-2.47.1\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\nydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed astor-0.8.1 blake3-1.0.6 cbor2-5.7.0 compressed-tensors-0.11.0 depyf-0.19.0 diskcache-5.6.3 fastapi-cli-0.0.13 fastapi-cloud-cli-0.2.1 gguf-0.17.1 httptools-0.6.4 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.11.3 mistral_common-1.8.5 msgspec-0.19.0 numba-0.61.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 openai-1.109.1 openai-harmony-0.0.4 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-extra-types-2.10.5 python-dotenv-1.1.1 pyzmq-27.1.0 ray-2.49.2 rich-toolkit-0.15.1 rignore-0.6.4 sympy-1.14.0 torch-2.8.0 torchaudio-2.8.0 torchvision-0.23.0 triton-3.4.0 uvloop-0.21.0 vllm-0.10.2 watchfiles-1.1.0 xformers-0.0.32.post1 xgrammar-0.1.23\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install logits_processor_zoo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T00:21:45.341080Z","iopub.execute_input":"2025-09-26T00:21:45.341821Z","iopub.status.idle":"2025-09-26T00:21:49.606510Z","shell.execute_reply.started":"2025-09-26T00:21:45.341789Z","shell.execute_reply":"2025-09-26T00:21:49.605351Z"}},"outputs":[{"name":"stdout","text":"Collecting logits_processor_zoo\n  Downloading logits_processor_zoo-0.2.1-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: accelerate>=0.26.1 in /usr/local/lib/python3.11/dist-packages (from logits_processor_zoo) (1.8.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from logits_processor_zoo) (2.8.0)\nRequirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from logits_processor_zoo) (4.56.2)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits_processor_zoo) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits_processor_zoo) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits_processor_zoo) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits_processor_zoo) (6.0.2)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits_processor_zoo) (0.35.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits_processor_zoo) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (4.14.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (1.13.1.3)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch->logits_processor_zoo) (3.4.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch->logits_processor_zoo) (75.2.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->logits_processor_zoo) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->logits_processor_zoo) (2.32.4)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->logits_processor_zoo) (0.22.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->logits_processor_zoo) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.1->logits_processor_zoo) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (2.4.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->logits_processor_zoo) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->logits_processor_zoo) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->logits_processor_zoo) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->logits_processor_zoo) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->logits_processor_zoo) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->logits_processor_zoo) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits_processor_zoo) (2024.2.0)\nDownloading logits_processor_zoo-0.2.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: logits_processor_zoo\nSuccessfully installed logits_processor_zoo-0.2.1\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"%%writefile inference.py\n\nimport random\nimport multiprocessing as mp\nimport numpy as np\nimport pandas as pd\nimport torch\nimport vllm\nimport os\n\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nfrom utils import build_dataset\nfrom constants import (\n    BASE_MODEL_PATH)\n\ndef _filter_single_token_aliases(tokenizer, candidates):\n    keep = []\n    for s in candidates:\n        ids = tokenizer.encode(s, add_special_tokens=False)\n        if len(ids) == 1:\n            keep.append(s)\n    return keep\n\n\ndef _fallback_single_token(tokenizer, word: str) -> str:\n    spaced = \" \" + word\n    if len(tokenizer.encode(spaced, add_special_tokens=False)) == 1:\n        return spaced\n    if len(tokenizer.encode(word, add_special_tokens=False)) == 1:\n        return word\n    return spaced\n\ndef run_inference_on_device(df_slice: pd.DataFrame, device_id: int) -> pd.DataFrame:\n    llm = vllm.LLM(\n        BASE_MODEL_PATH,\n        #quantization=\"awq\",\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.98,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2836,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        max_lora_rank=64,\n    )\n    \n    tokenizer = llm.get_tokenizer()\n    test_dataset = build_dataset(df_slice)\n    texts = test_dataset[\"prompt\"]\n    \n    yes_candidates = [\"Yes\", \" yes\", \"yes\", \"Yes.\", ...]\n    no_candidates  = [\"No\",  \" no\",  \"no\", \"No.\", ...]\n\n    yes_alias = _filter_single_token_aliases(tokenizer, yes_candidates)\n    no_alias  = _filter_single_token_aliases(tokenizer, no_candidates)\n    if not yes_alias:\n        yes_alias = [_fallback_single_token(tokenizer, POSITIVE_ANSWER)]\n    if not no_alias:\n        no_alias = [_fallback_single_token(tokenizer, NEGATIVE_ANSWER)]\n\n    choices = yes_alias + no_alias\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=choices)\n    outputs = llm.generate(\n        texts,\n        vllm.SamplingParams(\n            temperature=0.0,\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=len(choices),\n        ),\n        use_tqdm=True,\n    )\n\n    rows = []\n    for out, rid in zip(outputs, df_slice[\"row_id\"].values):\n        lp0 = {t.decoded_token: t.logprob for t in out.outputs[0].logprobs[0].values()}\n        p_yes = sum(np.exp(lp0.get(tok, -1e9)) for tok in yes_alias)\n        p_no  = sum(np.exp(lp0.get(tok, -1e9)) for tok in no_alias)\n        score = p_yes / (p_yes + p_no + 1e-12)\n        rows.append({\"row_id\": rid, \"rule_violation\": float(score)})\n    predictions = pd.DataFrame(rows, columns=[\"row_id\", \"rule_violation\"])\n    return predictions\n\ndef worker(device_id: int, df_slice: pd.DataFrame, return_dict):\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n    preds = run_inference_on_device(df_slice, device_id)\n    return_dict[device_id] = preds\n    \ndef main():\n    random.seed(42)\n    np.random.seed(42)\n    DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n\n    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n    test_dataframe[\"positive_example\"] = test_dataframe.apply(\n        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1\n    )\n    test_dataframe[\"negative_example\"] = test_dataframe.apply(\n        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1\n    )\n\n    test_dataframe = test_dataframe.drop(\n        columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n        errors=\"ignore\"\n    ).reset_index(drop=True)\n    n_gpus = max(1, torch.cuda.device_count())\n    n_procs = min(2, n_gpus)\n\n    df_slices = np.array_split(test_dataframe, n_procs)\n    for i in range(n_procs):\n        df_slices[i] = df_slices[i].reset_index(drop=True)\n\n    if n_procs == 1:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n        predictions = run_inference_on_device(df_slices[0], 0)\n    else:\n        manager = mp.Manager()\n        return_dict = manager.dict()\n        procs = []\n        for dev_id in range(n_procs):\n            p = mp.Process(target=worker, args=(dev_id, df_slices[dev_id], return_dict))\n            p.start()\n            procs.append(p)\n        for p in procs:\n            p.join()\n        predictions = pd.concat([return_dict[i] for i in range(n_procs)], ignore_index=True)\n    submission = predictions[[\"row_id\", \"rule_violation\"]].copy()\n    submission.to_csv(\"submission_qwen.csv\", index=False)\n    print(\"✅ Saved submission_qwen.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T00:26:00.330354Z","iopub.execute_input":"2025-09-26T00:26:00.330705Z","iopub.status.idle":"2025-09-26T00:26:00.339299Z","shell.execute_reply.started":"2025-09-26T00:26:00.330673Z","shell.execute_reply":"2025-09-26T00:26:00.338311Z"}},"outputs":[{"name":"stdout","text":"Overwriting inference.py\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!python train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:31:59.334686Z","iopub.execute_input":"2025-09-25T23:31:59.334986Z","iopub.status.idle":"2025-09-25T23:53:41.077362Z","shell.execute_reply.started":"2025-09-25T23:31:59.334966Z","shell.execute_reply":"2025-09-25T23:53:41.076151Z"}},"outputs":[{"name":"stdout","text":"2025-09-25 23:32:05.740418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758843125.765396     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758843125.773131     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n                                                 prompt completion\n0     \\nYou are given a comment from reddit and a ru...         no\n1     \\nYou are given a comment from reddit and a ru...         no\n2     \\nYou are given a comment from reddit and a ru...        yes\n3     \\nYou are given a comment from reddit and a ru...        yes\n4     \\nYou are given a comment from reddit and a ru...        yes\n...                                                 ...        ...\n2044  \\nYou are given a comment from reddit and a ru...         no\n2045  \\nYou are given a comment from reddit and a ru...         no\n2046  \\nYou are given a comment from reddit and a ru...         no\n2047  \\nYou are given a comment from reddit and a ru...         no\n2048  \\nYou are given a comment from reddit and a ru...         no\n\n[2049 rows x 2 columns]\nAdding EOS to train dataset: 100%|█| 2049/2049 [00:00<00:00, 19681.83 examples/s\nTokenizing train dataset: 100%|█████| 2049/2049 [00:03<00:00, 567.94 examples/s]\nPacking train dataset: 100%|██████| 2049/2049 [00:00<00:00, 23764.06 examples/s]\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.\n{'loss': 14.1257, 'grad_norm': 12.95374584197998, 'learning_rate': 4.908874981298057e-05, 'entropy': 1.7875691264867783, 'num_tokens': 78825.0, 'mean_token_accuracy': 0.0, 'epoch': 0.16}\n{'loss': 9.8792, 'grad_norm': 7.133109092712402, 'learning_rate': 4.2192486471335585e-05, 'entropy': 2.0824657022953033, 'num_tokens': 156148.0, 'mean_token_accuracy': 0.0, 'epoch': 0.33}\n{'loss': 8.7295, 'grad_norm': 4.213141918182373, 'learning_rate': 3.0374261005275607e-05, 'entropy': 2.5364128232002257, 'num_tokens': 234876.0, 'mean_token_accuracy': 0.0, 'epoch': 0.49}\n{'loss': 8.1709, 'grad_norm': 1.6537435054779053, 'learning_rate': 1.7017461746600506e-05, 'entropy': 3.1157343566417692, 'num_tokens': 314012.0, 'mean_token_accuracy': 0.0, 'epoch': 0.65}\n{'loss': 7.9675, 'grad_norm': 1.2801036834716797, 'learning_rate': 5.945948621809091e-06, 'entropy': 3.441216319799423, 'num_tokens': 393689.0, 'mean_token_accuracy': 0.0, 'epoch': 0.81}\n{'loss': 7.9094, 'grad_norm': 0.8781737089157104, 'learning_rate': 3.293369364618465e-07, 'entropy': 3.459987986087799, 'num_tokens': 472657.0, 'mean_token_accuracy': 0.0, 'epoch': 0.98}\n{'train_runtime': 1270.2085, 'train_samples_per_second': 0.387, 'train_steps_per_second': 0.049, 'train_loss': 9.413682306966473, 'entropy': 3.5554524660110474, 'num_tokens': 483534.0, 'mean_token_accuracy': 0.0, 'epoch': 1.0}\n100%|███████████████████████████████████████████| 62/62 [21:10<00:00, 20.49s/it]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!python inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T00:26:06.008990Z","iopub.execute_input":"2025-09-26T00:26:06.009754Z","iopub.status.idle":"2025-09-26T00:27:17.079353Z","shell.execute_reply.started":"2025-09-26T00:26:06.009726Z","shell.execute_reply":"2025-09-26T00:27:17.078200Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n[Worker 0] Running on GPU 0, data size=5\n[Worker 1] Running on GPU 1, data size=5\n2025-09-26 00:26:13.288205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-09-26 00:26:13.288203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758846373.317911     362 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758846373.318078     364 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758846373.327431     362 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1758846373.327500     364 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO 09-26 00:26:17 [__init__.py:216] Automatically detected platform cuda.\nINFO 09-26 00:26:17 [__init__.py:216] Automatically detected platform cuda.\nINFO 09-26 00:26:18 [utils.py:328] non-default args: {'trust_remote_code': True, 'dtype': 'half', 'max_model_len': 2836, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.98, 'disable_log_stats': True, 'enforce_eager': True, 'max_lora_rank': 64, 'model': '/kaggle/input/qwen-3-embedding/transformers/0.6b/1'}\nThe argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\nINFO 09-26 00:26:18 [utils.py:328] non-default args: {'trust_remote_code': True, 'dtype': 'half', 'max_model_len': 2836, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.98, 'disable_log_stats': True, 'enforce_eager': True, 'max_lora_rank': 64, 'model': '/kaggle/input/qwen-3-embedding/transformers/0.6b/1'}\nThe argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\nINFO 09-26 00:26:33 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n`torch_dtype` is deprecated! Use `dtype` instead!\nWARNING 09-26 00:26:33 [__init__.py:2767] Casting torch.bfloat16 to torch.float16.\nINFO 09-26 00:26:33 [__init__.py:1815] Using max model len 2836\nINFO 09-26 00:26:33 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n`torch_dtype` is deprecated! Use `dtype` instead!\nWARNING 09-26 00:26:33 [__init__.py:2767] Casting torch.bfloat16 to torch.float16.\nINFO 09-26 00:26:33 [__init__.py:1815] Using max model len 2836\nINFO 09-26 00:26:34 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\nINFO 09-26 00:26:34 [__init__.py:3400] Cudagraph is disabled under eager mode\nINFO 09-26 00:26:34 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\nINFO 09-26 00:26:34 [__init__.py:3400] Cudagraph is disabled under eager mode\nWARNING 09-26 00:26:35 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\nWARNING 09-26 00:26:35 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n2025-09-26 00:26:42.622923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758846402.650726     411 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758846402.659553     411 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-09-26 00:26:42.719307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758846402.747025     409 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758846402.755828     409 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO 09-26 00:26:46 [__init__.py:216] Automatically detected platform cuda.\nINFO 09-26 00:26:46 [__init__.py:216] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m INFO 09-26 00:26:47 [core.py:654] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m INFO 09-26 00:26:47 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/kaggle/input/qwen-3-embedding/transformers/0.6b/1', speculative_config=None, tokenizer='/kaggle/input/qwen-3-embedding/transformers/0.6b/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2836, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/kaggle/input/qwen-3-embedding/transformers/0.6b/1, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m INFO 09-26 00:26:47 [core.py:654] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m INFO 09-26 00:26:48 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/kaggle/input/qwen-3-embedding/transformers/0.6b/1', speculative_config=None, tokenizer='/kaggle/input/qwen-3-embedding/transformers/0.6b/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2836, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/kaggle/input/qwen-3-embedding/transformers/0.6b/1, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:26:49 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:26:49 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n[W926 00:27:00.154437444 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W926 00:27:00.158055982 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W926 00:27:10.165201763 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W926 00:27:10.165922125 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n[W926 00:27:10.168324000 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W926 00:27:10.168936949 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m INFO 09-26 00:27:10 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m INFO 09-26 00:27:10 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m WARNING 09-26 00:27:10 [logger.py:72] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m WARNING 09-26 00:27:10 [logger.py:72] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m INFO 09-26 00:27:10 [gpu_model_runner.py:2338] Starting to load model /kaggle/input/qwen-3-embedding/transformers/0.6b/1...\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m INFO 09-26 00:27:10 [gpu_model_runner.py:2338] Starting to load model /kaggle/input/qwen-3-embedding/transformers/0.6b/1...\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m INFO 09-26 00:27:11 [gpu_model_runner.py:2370] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m INFO 09-26 00:27:11 [gpu_model_runner.py:2370] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m INFO 09-26 00:27:11 [logger.py:66] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m INFO 09-26 00:27:11 [logger.py:66] Using FlexAttention backend on V1 engine.\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718] EngineCore failed to start.\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718] Traceback (most recent call last):\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 505, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 82, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.model_executor = executor_class(vllm_config)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self._init_executor()\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 49, in _init_executor\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.collective_rpc(\"load_model\")\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     answer = run_method(self.driver_worker, method, args, kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils/__init__.py\", line 3060, in run_method\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     return func(*args, **kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 2371, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.model = model_loader.load_model(\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 50, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.load_weights(model, model_config)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/default_loader.py\", line 265, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     loaded_weights = model.load_weights(\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                      ^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3.py\", line 344, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     return loader.load_weights(weights)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 291, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 277, in _load_module\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     raise ValueError(msg)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718] ValueError: There is no module or parameter named 'embed_tokens' in Qwen3ForCausalLM\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m Process EngineCore_DP0:\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m Traceback (most recent call last):\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     self.run()\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 722, in run_engine_core\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     raise e\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 505, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 82, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     self._init_executor()\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 49, in _init_executor\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     self.collective_rpc(\"load_model\")\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils/__init__.py\", line 3060, in run_method\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 2371, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 50, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/default_loader.py\", line 265, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     loaded_weights = model.load_weights(\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3.py\", line 344, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     return loader.load_weights(weights)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 291, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 277, in _load_module\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m     raise ValueError(msg)\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m ValueError: There is no module or parameter named 'embed_tokens' in Qwen3ForCausalLM\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:01<?, ?it/s]\n\u001b[1;36m(EngineCore_DP0 pid=411)\u001b[0;0m \n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718] EngineCore failed to start.\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718] Traceback (most recent call last):\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 505, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 82, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.model_executor = executor_class(vllm_config)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self._init_executor()\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 49, in _init_executor\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.collective_rpc(\"load_model\")\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     answer = run_method(self.driver_worker, method, args, kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils/__init__.py\", line 3060, in run_method\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     return func(*args, **kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 2371, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.model = model_loader.load_model(\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 50, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     self.load_weights(model, model_config)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/default_loader.py\", line 265, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     loaded_weights = model.load_weights(\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                      ^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3.py\", line 344, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     return loader.load_weights(weights)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 291, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 277, in _load_module\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718]     raise ValueError(msg)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ERROR 09-26 00:27:12 [core.py:718] ValueError: There is no module or parameter named 'embed_tokens' in Qwen3ForCausalLM\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m Process EngineCore_DP0:\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m Traceback (most recent call last):\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     self.run()\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 722, in run_engine_core\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     raise e\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 505, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 82, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     self._init_executor()\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 49, in _init_executor\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     self.collective_rpc(\"load_model\")\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils/__init__.py\", line 3060, in run_method\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 2371, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 50, in load_model\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/default_loader.py\", line 265, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     loaded_weights = model.load_weights(\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3.py\", line 344, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     return loader.load_weights(weights)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 291, in load_weights\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 277, in _load_module\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m     raise ValueError(msg)\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m ValueError: There is no module or parameter named 'embed_tokens' in Qwen3ForCausalLM\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:01<?, ?it/s]\n\u001b[1;36m(EngineCore_DP0 pid=409)\u001b[0;0m \n[rank0]:[W926 00:27:13.488568495 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n[rank0]:[W926 00:27:14.510230656 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nProcess Process-3:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/kaggle/working/inference.py\", line 88, in worker\n    preds = run_inference_on_device(df_slice, device_id)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/inference.py\", line 33, in run_inference_on_device\n    llm = vllm.LLM(\n          ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\", line 282, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\", line 493, in from_engine_args\n    return engine_cls.from_vllm_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/llm_engine.py\", line 134, in from_vllm_config\n    return cls(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/llm_engine.py\", line 111, in __init__\n    self.engine_core = EngineCoreClient.make_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\", line 80, in make_client\n    return SyncMPClient(vllm_config, executor_class, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\", line 602, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\", line 448, in __init__\n    with launch_core_engines(vllm_config, executor_class,\n  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/utils.py\", line 729, in launch_core_engines\n    wait_for_engine_startup(\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/utils.py\", line 782, in wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\nProcess Process-2:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/kaggle/working/inference.py\", line 88, in worker\n    preds = run_inference_on_device(df_slice, device_id)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/inference.py\", line 33, in run_inference_on_device\n    llm = vllm.LLM(\n          ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\", line 282, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\", line 493, in from_engine_args\n    return engine_cls.from_vllm_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/llm_engine.py\", line 134, in from_vllm_config\n    return cls(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/llm_engine.py\", line 111, in __init__\n    self.engine_core = EngineCoreClient.make_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\", line 80, in make_client\n    return SyncMPClient(vllm_config, executor_class, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\", line 602, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\", line 448, in __init__\n    with launch_core_engines(vllm_config, executor_class,\n  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/utils.py\", line 729, in launch_core_engines\n    wait_for_engine_startup(\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/utils.py\", line 782, in wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\nTraceback (most recent call last):\n  File \"/kaggle/working/inference.py\", line 134, in <module>\n    main()\n  File \"/kaggle/working/inference.py\", line 128, in main\n    predictions = pd.concat([return_dict[i] for i in range(n_procs)], ignore_index=True)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/inference.py\", line 128, in <listcomp>\n    predictions = pd.concat([return_dict[i] for i in range(n_procs)], ignore_index=True)\n                             ~~~~~~~~~~~^^^\n  File \"<string>\", line 2, in __getitem__\n  File \"/usr/lib/python3.11/multiprocessing/managers.py\", line 837, in _callmethod\n    raise convert_to_error(kind, result)\nKeyError: 0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}